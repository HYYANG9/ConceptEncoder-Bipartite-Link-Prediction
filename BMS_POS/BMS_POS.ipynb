{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a217b485-2c77-4334-b631-9d5c1557e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from math import sqrt as msqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, classification_report\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adadelta\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c28064-03f2-4a67-a5c7-0786036ed377",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e8972-4cb3-487f-b989-af4bc8d59d71",
   "metadata": {},
   "source": [
    "## Training Set-Concept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8f5558-cd64-4c03-a818-2ebd6295db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract extents and intents from txt file\n",
    "def get_intents_extents(filename):\n",
    "    intents = []\n",
    "    extents = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line based on four blank spaces\n",
    "            parts = line.split('    ')\n",
    "\n",
    "            if len(parts) == 2:\n",
    "                intent = parts[0].strip()\n",
    "                intents.append(intent)\n",
    "                \n",
    "\n",
    "                extent = parts[1].strip()\n",
    "                extents.append(extent)\n",
    "                \n",
    "        modified_intents = [' '.join(['a' + token for token in item.split()]) for item in intents]\n",
    "        modified_extents = [' '.join(['o' + token for token in item.split()]) for item in extents]\n",
    "\n",
    "    return modified_intents, modified_extents, intents, extents\n",
    "\n",
    "# The function to process formal context file\n",
    "def process_context(file_name):\n",
    "    intents = []\n",
    "    extents = []\n",
    "    modified_intents = []\n",
    "    modified_extents = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                extents.append(parts[0])\n",
    "                intents.append(parts[1])\n",
    "                modified_extents.append('o{}'.format(parts[0]))\n",
    "                modified_intents.append('a{}'.format(parts[1]))\n",
    "    return extents, intents, modified_extents, modified_intents\n",
    "\n",
    "# Truncate concept, delete concepts that are longer than \"max_length\"\n",
    "def truncate_concept(concept, max_len):\n",
    "    \n",
    "    max_length = 0\n",
    "    longest_sequence = []\n",
    "\n",
    "    for sequence in concept:\n",
    "        num_elements = len(sequence.split())\n",
    "        if num_elements >= max_length:\n",
    "            max_length = num_elements\n",
    "            longest_sequence = sequence\n",
    "\n",
    "    print(\"Length of the longest sequence:\", max_length)\n",
    "    \n",
    "    truncated_concept = [sequence for sequence in concept if len(sequence) <= max_len]\n",
    "    print(\"The number of concepts with limited length is:\", len(truncated_concept))\n",
    "    \n",
    "          \n",
    "    return truncated_concept, max_length\n",
    "\n",
    "# Use objects and attribtutes to crate index dictionary\n",
    "def create_index_dic(filename):\n",
    "    full_extents, full_intents, test_extents, test_intents = process_context(filename)\n",
    "    object_list = list(set(\" \".join(full_extents).split()))\n",
    "    sorted_object_list = sorted(map(int, object_list))\n",
    "    \n",
    "    attribute_list = list(set(\" \".join(full_intents).split()))\n",
    "    sorted_attribute_list = sorted(map(int, attribute_list))\n",
    "    \n",
    "    special_tokens = {'[PAD]': 0, '[CLS]': 1 }\n",
    "    object2idx = {'o' + str(obj): int(obj)+2  for  obj in sorted_object_list}\n",
    "    attribute2idx = {'a' + str(att): int(att)+10003  for  att in sorted_attribute_list}\n",
    "    \n",
    "    index = {}\n",
    "    index.update(special_tokens)\n",
    "    index.update(object2idx)\n",
    "    index.update(attribute2idx)\n",
    "    \n",
    "    return index, object2idx, attribute2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e6ea44-c04a-4433-a027-5f97ca94b98c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training intents is 5247\n",
      "The number of training extents is 5247\n"
     ]
    }
   ],
   "source": [
    "train_intents, train_extents, original_train_intents, original_train_extents = get_intents_extents('BMS-POS-with-missing-part_concepts.txt')\n",
    "\n",
    "print('The number of training intents is', len(train_intents))\n",
    "print('The number of training extents is', len(train_extents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64cfa42-2584-496b-85f2-c5487633c378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest sequence: 6\n",
      "The number of concepts with limited length is: 5247\n",
      "Length of the longest sequence: 200\n",
      "The number of concepts with limited length is: 5247\n",
      "max_len_ext = 200\n",
      "max_len_int = 6\n",
      "max_len = 200\n"
     ]
    }
   ],
   "source": [
    "# The maximum length\n",
    "max_len_int = 10240\n",
    "max_len_ext = 10240\n",
    "max_len = 1024\n",
    "# Generate the truncated concept \n",
    "truncated_intent, original_max_len_int = truncate_concept(train_intents, max_len_int)\n",
    "truncated_extent, original_max_len_ext = truncate_concept(train_extents, max_len_ext)\n",
    "\n",
    "if max_len_int >= original_max_len_int:\n",
    "    max_len_int = original_max_len_int\n",
    "    \n",
    "if max_len_ext >= original_max_len_ext:\n",
    "    max_len_ext = original_max_len_ext\n",
    "\n",
    "if max_len_ext >= max_len_int:\n",
    "    max_len = max_len_ext\n",
    "else:\n",
    "        max_len = max_len_int\n",
    "        \n",
    "print('max_len_ext =',max_len_ext)\n",
    "print('max_len_int =',max_len_int)\n",
    "print('max_len =',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566c9b16-8fe0-49c5-8a2c-0967e6a104ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of indices is  10839\n"
     ]
    }
   ],
   "source": [
    "# Generate index dictionary\n",
    "index_dic, obj_dic, att_dic = create_index_dic('BMS-POS-with-missing-part.txt')\n",
    "\n",
    "# print(index_dic)\n",
    "print('The number of indices is ', len(index_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36578f5-db0a-42fc-a202-285e115e0df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest number in the object index is  10002\n",
      "The largest number in the attribute index is  11007\n"
     ]
    }
   ],
   "source": [
    "last_item_obj = list(obj_dic.items())[-1]\n",
    "max_index_obj = last_item_obj[1] \n",
    "print(\"The largest number in the object index is \",max_index_obj)\n",
    "\n",
    "last_item_att = list(att_dic.items())[-1]\n",
    "max_index_att = last_item_att[1] \n",
    "print(\"The largest number in the attribute index is \",max_index_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3699e5-85c2-44cd-8ab3-e089763ec231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n"
     ]
    }
   ],
   "source": [
    "org_intent_token_list = []\n",
    "\n",
    "for sequence in truncated_intent:\n",
    "    int_tokens = sequence.split()  # Split sequence into tokens\n",
    "    indices = [index_dic[token] for token in int_tokens if token in index_dic]  # Convert tokens to indices\n",
    "    org_intent_token_list.append(indices)  # Store indices in concept_token_list\n",
    "\n",
    "print(len(org_intent_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e93aff-b360-4a11-b0f1-4a021ae545ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n"
     ]
    }
   ],
   "source": [
    "org_extent_token_list = []\n",
    "\n",
    "for sequence in truncated_extent:\n",
    "    ext_tokens = sequence.split()  # Split sequence into tokens\n",
    "    indices = [index_dic[token] for token in ext_tokens if token in index_dic]  # Convert tokens to indices\n",
    "    org_extent_token_list.append(indices)  # Store indices in concept_token_list\n",
    "\n",
    "print(len(org_extent_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cc9939-6cba-4581-ad04-cb5e0691f3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n"
     ]
    }
   ],
   "source": [
    "org_ext_positive = org_extent_token_list\n",
    "org_int_positive = org_intent_token_list\n",
    "print(len(org_int_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa790c4-b73d-43d0-ab22-b57a3e34393f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_negative_sequence(sequence, min_index, max_index):\n",
    "    length = len(sequence)\n",
    "    num_elements_to_replace = max(1, int(length * .15)) # Replace 15% elements\n",
    "    \n",
    "    # Copy the original sequence\n",
    "    new_sequence = sequence[:]\n",
    "    \n",
    "    # Replace 15% of elements with random numbers\n",
    "    for _ in range(num_elements_to_replace):\n",
    "        index_to_replace = random.randint(0, length - 1)\n",
    "        new_sequence[index_to_replace] = random.randint(min_index, max_index)\n",
    "    \n",
    "    return new_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603d7f7a-1478-49c9-8248-5bdea6cdbf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "org_ext_negative = []\n",
    "min_index_obj = 4\n",
    "for sequence in org_ext_positive:\n",
    "    if len(sequence) >= 2:\n",
    "        for _ in range(1):  # Generate 3 negative samples for each positive sample\n",
    "            new_sequence = generate_negative_sequence(sequence, min_index_obj,  max_index_obj)\n",
    "            \n",
    "            # Ensure the new sequence is different from any sequences in positive_samples and negative_samples\n",
    "            while new_sequence in org_ext_positive or new_sequence in org_ext_negative:\n",
    "                new_sequence = generate_negative_sequence(sequence, min_index_obj, max_index_obj)\n",
    "            \n",
    "            org_ext_negative.append(new_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae3ae73-d5f1-408f-9814-a343634afdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5076\n",
      "[[10096, 10351]]\n"
     ]
    }
   ],
   "source": [
    "org_int_negative = []\n",
    "min_index_att = max_index_obj + 1\n",
    "\n",
    "for sequence in org_int_positive:\n",
    "    if len(sequence) >= 2:\n",
    "        for _ in range(1):  # Generate 3 negative samples for each positive sample\n",
    "            new_sequence = generate_negative_sequence(sequence, min_index_att, max_index_att)\n",
    "            \n",
    "            # Ensure the new sequence is different from any sequences in positive_samples and negative_samples\n",
    "            while new_sequence in org_int_positive or new_sequence in org_int_negative:\n",
    "                new_sequence = generate_negative_sequence(sequence, min_index_att, max_index_att)\n",
    "            \n",
    "            org_int_negative.append(new_sequence)\n",
    "        \n",
    "print(len(org_int_negative))\n",
    "\n",
    "print(org_int_negative[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4bf26a-e844-4298-a96b-23c729399b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n"
     ]
    }
   ],
   "source": [
    "while len(org_int_negative) < len(org_int_positive):\n",
    "    org_int_negative.append([0])\n",
    "print(len(org_int_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b09c78-d1b5-45fb-ae1b-8b86e7d9fe10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences_with_zeros(token_list, max_len):\n",
    "    padded_sequences = []\n",
    "    for sequence in token_list:\n",
    "        pad_sequence_int = sequence + [0] * (max_len - len(sequence))\n",
    "        padded_sequences.append(pad_sequence_int)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c82c138-df6c-42bf-ac44-d2aa0fd54f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n",
      "5247\n",
      "5247\n",
      "5247\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences with zeros\n",
    "int_pos = pad_sequences_with_zeros(org_int_positive, max_len)\n",
    "int_neg = pad_sequences_with_zeros(org_int_negative, max_len)\n",
    "ext_pos = pad_sequences_with_zeros(org_ext_positive, max_len)\n",
    "ext_neg = pad_sequences_with_zeros(org_ext_negative, max_len)\n",
    "\n",
    "print(len(int_pos))\n",
    "print(len(int_neg))\n",
    "print(len(ext_pos))\n",
    "print(len(ext_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3834d178-a357-4c65-aa52-ef3836e38447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_test_data(positive_token_list, negative_sample_list):\n",
    "    # Generate labels for positive samples (1) and negative samples (0)\n",
    "    positive_labels = [1] * len(positive_token_list)\n",
    "    negative_labels = [0] * len(negative_sample_list)\n",
    "\n",
    "    # Combine sequences and labels\n",
    "    samples = positive_token_list + negative_sample_list\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    # Verify the lengths to ensure they match\n",
    "    assert len(samples) == len(labels)\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e69d6d-3d5c-430f-ae8c-4630c2a6abcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_extent_set, train_extent_labels = generate_training_test_data(ext_pos, ext_neg)\n",
    "train_intent_set, train_intent_labels = generate_training_test_data(int_pos, int_neg)\n",
    "# print(train_intent_set[:1])\n",
    "# print(len(train_intent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f79b7f-9083-4e13-9f1a-4c8610970388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_concept_set = [train_extent_set[i] + train_intent_set[i] for i in range(len(train_intent_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4996c1fd-7977-47c9-8819-d1b320473ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10494\n",
      "10494\n"
     ]
    }
   ],
   "source": [
    "train_concept_labels = train_extent_labels\n",
    "print(len(train_concept_set))\n",
    "print(len(train_concept_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c220b4-bb62-4a3e-b3c2-b7af8e40c363",
   "metadata": {},
   "source": [
    "## Training Set-Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a46370-a34b-4c77-889c-7898fc808efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function to process formal context file\n",
    "def process_context(file_name):\n",
    "    incidence = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                incidence.append('o{} a{}'.format(parts[0], parts[1]))\n",
    "    return incidence\n",
    "\n",
    "# Function to generate negative samples\n",
    "def generate_negative_samples(number_of_samples, existing_token_list, obj_dic, att_dic, index_dic):\n",
    "    negative_sample_list = []\n",
    "    \n",
    "    # Create a set of existing sequences for faster lookup\n",
    "    existing_sequences = set(tuple(seq) for seq in existing_token_list)\n",
    "    \n",
    "    # Iterate until the number of negative samples matches the number of positive samples\n",
    "    while len(negative_sample_list) < number_of_samples :\n",
    "        # Randomly select an object and attribute\n",
    "        obj = random.choice(list(obj_dic.keys()))\n",
    "        att = random.choice(list(att_dic.keys()))\n",
    "        \n",
    "        # Create a sequence from the randomly selected object and attribute\n",
    "        sequence = [obj_dic[obj], att_dic[att]]\n",
    "        \n",
    "        # Check if the sequence is not in the existing sequences and not in the negative sample list\n",
    "        if tuple(sequence) not in existing_sequences and tuple(sequence) not in negative_sample_list:\n",
    "            negative_sample_list.append(sequence)\n",
    "    \n",
    "    return negative_sample_list\n",
    "\n",
    "def generate_training_test_data(positive_token_list, negative_sample_list):\n",
    "    # Generate labels for positive samples (1) and negative samples (0)\n",
    "    positive_labels = [1] * len(positive_token_list)\n",
    "    negative_labels = [0] * len(negative_sample_list)\n",
    "\n",
    "    # Combine sequences and labels\n",
    "    samples = positive_token_list + negative_sample_list\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    # Verify the lengths to ensure they match\n",
    "    assert len(samples) == len(labels)\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a17863-98d4-4512-a29f-8b1985de9427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of incidences in training set is : 63421\n"
     ]
    }
   ],
   "source": [
    "# Load the file and process its contents\n",
    "FT_incidence = process_context('BMS-POS-with-missing-part.txt')\n",
    "\n",
    "print('The number of incidences in training set is :',len(FT_incidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e0b43b1-a8d5-4110-bbab-3649eadc2c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different elements: 10837\n"
     ]
    }
   ],
   "source": [
    "unique_elements = set()\n",
    "\n",
    "for seq in FT_incidence:\n",
    "    unique_elements.update(seq.split())\n",
    "\n",
    "num_unique_elements = len(unique_elements)\n",
    "print(\"Number of different elements:\", num_unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02833770-9783-47d9-bfbf-297e14304c66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive data is : 63421\n"
     ]
    }
   ],
   "source": [
    "# Convert training set sequences to index\n",
    "FT_positive_token_list = []\n",
    "for incidence in FT_incidence:\n",
    "    tokens = incidence.split()\n",
    "    if tokens[0] in index_dic and tokens[1] in index_dic:\n",
    "        FT_positive_token_list.append([\n",
    "            index_dic[tokens[0]],\n",
    "            index_dic[tokens[1]]\n",
    "        ])\n",
    "print('The number of positive data is :', len(FT_positive_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19dedfdb-1a8a-4e64-8885-e1cfd5925b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of negative data is : 63421\n",
      "Length of train_context_set: 126842\n",
      "Length of train_context_labels: 126842\n"
     ]
    }
   ],
   "source": [
    "FT_negative_sample_list = generate_negative_samples(len(FT_positive_token_list), FT_positive_token_list, obj_dic, att_dic, index_dic)\n",
    "\n",
    "# Print the negative samples\n",
    "# print(negative_sample_list)\n",
    "print('The number of negative data is :', len(FT_negative_sample_list))\n",
    "\n",
    "org_train_context_set, train_context_labels = generate_training_test_data(FT_positive_token_list, FT_negative_sample_list)\n",
    "print(\"Length of train_context_set:\", len(org_train_context_set))\n",
    "print(\"Length of train_context_labels:\", len(train_context_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80c671e4-2de7-4d69-8887-9724871f5cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences_with_zeros(context_token, max_len):\n",
    "    padded_sequences = []\n",
    "    for sequence in context_token:\n",
    "        padded_sequence = sequence[:1] + [0] * (max_len - 1) + sequence[1:] + [0] * (max_len - 1)\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2c7ec50-8d0e-4a63-98bb-3a9d39771cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 126842\n",
      "Length of train labels: 126842\n"
     ]
    }
   ],
   "source": [
    "train_context_set = pad_sequences_with_zeros(org_train_context_set, max_len)\n",
    "# train_set = train_concept_set + train_context_set\n",
    "# train_labels = train_concept_labels + train_context_labels\n",
    "\n",
    "train_set =  train_context_set\n",
    "train_labels = train_context_labels\n",
    "\n",
    "print(\"Length of train set:\", len(train_set))\n",
    "print(\"Length of train labels:\", len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e33db-761c-4c9c-9c08-0a030501a2d0",
   "metadata": {},
   "source": [
    "## TEST Set-Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8fa5b86-a5f7-4a96-a803-ff882fcb4190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the file and process its contents\n",
    "TEST_incidence = process_context('BMS-POS.txt')\n",
    "\n",
    "# Convert training set sequences to index\n",
    "test_token_list = []\n",
    "for incidence in TEST_incidence:\n",
    "    tokens_test = incidence.split()\n",
    "    if tokens_test[0] in index_dic and tokens_test[1] in index_dic:\n",
    "        test_token_list.append([\n",
    "            index_dic[tokens_test[0]],\n",
    "            index_dic[tokens_test[1]]\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85c60d67-4217-472f-97f8-acc953b85bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_set: 13832\n",
      "Length of test_labels: 13832\n"
     ]
    }
   ],
   "source": [
    "# Convert inner lists into tuples\n",
    "FT_token_set = set(tuple(seq) for seq in FT_positive_token_list)\n",
    "test_token_set = set(tuple(seq) for seq in test_token_list)\n",
    "\n",
    "# Find the sequences that are in token_set but not in incidence_token_set\n",
    "test_positive_samples = [list(seq) for seq in (test_token_set - FT_token_set)]\n",
    "all_positive_samples = [list(seq) for seq in test_token_set]\n",
    "\n",
    "# Check if the population size is smaller than 1000\n",
    "# if len(positive_samples) > 500:\n",
    "\n",
    "#     test_positive_samples = random.sample(positive_samples, 500)  # Sample 1000 sequences\n",
    "\n",
    "\n",
    "test_negative_samples = generate_negative_samples(len(test_positive_samples), all_positive_samples, obj_dic, att_dic, index_dic)\n",
    "\n",
    "org_test_set, test_labels = generate_training_test_data(test_positive_samples, test_negative_samples)\n",
    "\n",
    "test_set = pad_sequences_with_zeros(org_test_set, max_len)\n",
    "print(\"Length of test_set:\", len(org_test_set))\n",
    "print(\"Length of test_labels:\", len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786e498-2b09-4718-bb8e-c02df992830e",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6e8-fa3a-40f6-95db-57ef1f98f8b2",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e71abe12-dd27-4fd4-89a3-faf602558f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dimension of key, values. the dimension of query and key are the same \n",
    "d_k = d_v = 64\n",
    "# dimension of embedding\n",
    "d_model = 768  # n_heads * d_k\n",
    "# dimension of hidden layers\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# number of heads\n",
    "n_heads = 12\n",
    "# number of encoders\n",
    "n_layers = 9\n",
    "# # the number of input setences\n",
    "n_segs = 1\n",
    "p_dropout = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250449b-7ba4-4997-bf60-7cf5c9b21a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaylines{\n",
    "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
    " or \\\\\n",
    "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2afabc0f-b617-4ada-980c-4ab6374e2741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''\n",
    "    Two way to implements GELU:\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    or\n",
    "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "    '''\n",
    "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "\n",
    "#  create a mask tensor to identify the padding tokens in a batch of sequences\n",
    "def get_pad_mask(tokens, pad_idx=0):\n",
    "    '''\n",
    "    suppose index of [PAD] is zero in index dictionary\n",
    "    the size of input tokens is [batch, seq_len]\n",
    "    '''\n",
    "    batch, seq_len = tokens.size()\n",
    "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) #.unsqueeze(1) adds a dimension and turns it to column vectors\n",
    "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
    "    \n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbdfc5f8-5107-4d28-ae6f-e3cf435ff501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process input tokens to dense vectors before passing them to encoder.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,max_vocab, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
    "        '''\n",
    "        convert indices into vector embeddings.\n",
    "        max_vocab can be replaced by formal context object vectors or attribute vectors\n",
    "        '''\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        # print(\"Input to Embeddings.forward - x:\", x.size())\n",
    "        word_enc = self.word_emb(x)\n",
    "        # print(\"Output from Embeddings.forward - word_enc:\", word_enc.size())\n",
    "\n",
    "        # seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918d4e8-120a-475a-8347-9b2819931e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
    "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e04a1a-6b7a-4b33-a880-a04b4c603f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        # fill the positions in the scores tensor where the attn_mask is True with a very large negative value (-1e9). \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q, K, V: [batch, seq_len, d_model]\n",
    "        attn_mask: [batch, seq_len, seq_len]\n",
    "        '''\n",
    "        batch = Q.size(0)\n",
    "        '''\n",
    "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
    "        Convenient for matrix multiply opearation later\n",
    "        q, k, v: [batch, n_heads, seq_len, d_k or d_v]\n",
    "        '''\n",
    "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
    "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch, n_heads, seq_len, d_v]\n",
    "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch, -1, n_heads * d_v)\n",
    "\n",
    "        # output: [batch, seq_len, d_model]\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e55b-316c-4f61-9cd2-b2a4e345c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\operatorname{FFN}(x)=\\operatorname{GELU}(xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6778e9cd-6b44-4da2-bb4d-13be02ba6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68c1f620-2e47-45d3-86c6-e6d3c57afde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        '''\n",
    "        pre-norm\n",
    "        x: [batch, seq_len, d_model]\n",
    "        '''\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de7edf80-fca4-4bf1-b8c4-84518a76d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next sentence prediction\n",
    "# pooled representation of the entire sequence as the [CLS] token representation.\n",
    "'''\n",
    "The full connected linear layer improve the result while making the model harder to train.\n",
    "'''\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec84a485-9754-4307-ab05-b896718e330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, max_vocab, max_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embeddings(max_vocab, max_len)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pooler = Pooler()\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        output = self.embedding(tokens)\n",
    "        enc_self_pad_mask = get_pad_mask(tokens)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, enc_self_pad_mask)\n",
    "        # output: [batch, max_len, d_model]\n",
    "        '''\n",
    "        Extracting the [CLS] token representation, \n",
    "        passing it through the pooler, \n",
    "        and making predictions.\n",
    "        '''\n",
    "        # hidden_pool = self.pooler(output[:, 0]) # only the [CLS] token\n",
    "        hidden_pool = self.pooler(torch.mean(output, 1))\n",
    "\n",
    "        return hidden_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911702b-adcb-4dd3-b7ca-c965aceaf4f3",
   "metadata": {},
   "source": [
    "## Input Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5afe0534-d54a-4aa2-9aaa-384813468cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data(concepts, index_dic, max_vocab, max_len):\n",
    "    batch_data = []\n",
    "\n",
    "    for concpet_tokens in concepts :       \n",
    "\n",
    "        input_ids = [index_dic['[CLS]']] + concpet_tokens\n",
    "        batch_data.append([input_ids])\n",
    "\n",
    "    random.shuffle(batch_data)\n",
    "\n",
    "    return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f0d2e3a-cffe-4842-8f8d-6c4721d5ac18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest number in the index is  11007\n"
     ]
    }
   ],
   "source": [
    "last_item = list(index_dic.items())[-1]\n",
    "print(\"The largest number in the index is \",last_item[1])\n",
    "\n",
    "max_vocab = last_item[1] + 1\n",
    "\n",
    "max_len = max_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dea7d46d-d8ef-4480-a6d1-4ffd3d0c5e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef9943-2c2e-4e63-9d0e-4d98973caddb",
   "metadata": {},
   "source": [
    "##  MLP for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1afb7f61-7896-43af-bcab-e39ed0914367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design a MLP for classification task\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, encoder_model1, encoder_model2, embedding_size, hidden_size, output_size, dropout_rate = .1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.encoder1 = encoder_model1\n",
    "        self.encoder2 = encoder_model2\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_size * 2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, objs, attrs):\n",
    "        x1 = self.encoder1(objs)\n",
    "        x2 = self.encoder2(attrs)\n",
    "        x = self.fc1(torch.cat((x1, x2), dim=1))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7350f22-1a19-4070-9848-beab8c82ac1a",
   "metadata": {},
   "source": [
    "# Training & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d50d109-a6e5-41c6-a451-43391b326a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "learning_rate = 9e-6\n",
    "num_epochs = 120\n",
    "batch_size = 32\n",
    "\n",
    "# Load pre-trained model\n",
    "model1 = Encoder(n_layers, max_vocab, max_len)\n",
    "model2 = Encoder(n_layers, max_vocab, max_len)\n",
    "\n",
    "model1.train().to(device)\n",
    "model2.train().to(device)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MLP_model = MLP(model1, model2, d_model, hidden_size, output_size, dropout_rate=0.1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "train_inputs = torch.tensor(train_set).to(device)\n",
    "train_labels = torch.tensor(train_labels).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_inputs = torch.tensor(test_set).to(device)\n",
    "test_labels = torch.tensor(test_labels).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a76fa74-69ff-4214-b7da-94c646012c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    MLP_model.train()\n",
    "    MLP_model.encoder1.train()\n",
    "    MLP_model.encoder2.train()\n",
    "    \n",
    "    # ======================== Training =====================================\n",
    "   \n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # objs, attrs = torch.tensor_split(inputs, [1], dim = 1)\n",
    "        objs, attrs = torch.tensor_split(inputs, [max_len+1], dim = 1)\n",
    "        \n",
    "        outputs = MLP_model(objs, attrs)\n",
    "        # print(outputs.size())\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    # Print the training loss\n",
    "    print(f'Epoch:{epoch + 1} \\t loss: {loss:.3f}')\n",
    "        \n",
    "    # ======================== Running test case =====================================\n",
    "    # Switch the model to evaluate mode\n",
    "    MLP_model.eval()\n",
    "    MLP_model.encoder1.eval()\n",
    "    MLP_model.encoder2.eval()\n",
    "\n",
    "    # Initialize lists to store predictions and labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in test_loader:\n",
    "            # objs, attrs = torch.tensor_split(batch_inputs, [1], dim = 1)\n",
    "            objs, attrs = torch.tensor_split(inputs, [max_len+1], dim = 1)\n",
    "                \n",
    "            # Get predictions\n",
    "            test_outputs = MLP_model(objs, attrs)\n",
    "            predictions = (test_outputs > 0.5).float().cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "            # Convert labels to numpy\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Convert predictions to binary (0 or 1)\n",
    "    predictions_binary = (all_predictions > 0.5).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, predictions_binary)\n",
    "    precision = precision_score(all_labels, predictions_binary)\n",
    "    recall = recall_score(all_labels, predictions_binary)\n",
    "    f1 = f1_score(all_labels, predictions_binary)\n",
    "    auc = roc_auc_score(all_labels, all_predictions)\n",
    "    aupr = average_precision_score(all_labels, all_predictions)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Test Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, AUPR: {aupr:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a64b9-c4a6-4fa9-a9f3-09744a07232d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
